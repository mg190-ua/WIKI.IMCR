# Descriptores 2D.

Los descriptores 2D, en el mundo del procesamiento de imágenes, son técnicas que se usan para describir características o *features* en una imagen para poder identificarla. Son muy útiles en el procesamiento de imágenes para clasificarlas, por ejemplo, o reconocer objetos en la propia imagen, entre otros.

## Tipos de descriptores.

Actualmente existen varios tipos de descriptores que podemos usar para nuestro proyecto. Aquí veremos seis, aunque dos sólo serán nombrados pues forman parte de otros dos que se explicarán.

### SIFT

*Scale-Invariant Feature Transform* es un descriptor invariante en cambios de escala, rotación e iluminación. Detecta puntos de interés y características locales en una imagen utilizando histogramas de orientación de gradientes o *HOG*.

Para tener claro qué es un *HOG*, a continuación se muestra una imagen que explica cómo estos descriptores reconocen patrones en una imagen. En este ejemplo, se usa un patrón extraído a partir de muchas imágenes de caras para reconocer una cara en una imagen dada.

![image](Images/ImagenesDescriptores/hog.png)

### SURF

*Speeded Up Robust Features* es similar a SIFT en que es invariante a cambios de escala y rotación y utiliza histogramas de orientación de gradientes. No obstante, es más rápido debido a que utiliza aproximaciones a los cálculos de gradientes y cálculos de determinantes de matrices hessianas.

Una matriz hessiana es una matriz cuadrada de segundas derivadas parciales de una función escalar. En este contexto se utiliza para analizar la estructura local de la imagen en un punto de interés.

### ORB

*Oriented FAST and Rotated BRIEF* utiliza una combinación de características de detección y descripción. ORB usa el algoritmo de detección de puntos clave *FAST* para detectar puntos de interés, y luego utiliza el descriptor *BRIEF* para describir las características locales en una imagen. ORB también es invariante a cambios de escala y rotación.

### BRISK

*Binary Robust Invariant Scalable Keypoints* es una versión mejorada y más rápida de ORB, y utiliza la misma estrategia de detección y descripción. BRISK utiliza una aproximación binaria para la descripción de características, lo que reduce el tamaño del descriptor y aumenta la eficiencia. Es robusto frente a cambios de escala.

## Práctica.

Vamos a ver cómo podemos usar nosotros estos descriptores para nuestro beneficio. Deberemos tener MATLAB para poder llevar a cabo este ejemplo introductorio. 

Para esta práctica usaremos las siguientes dos imágenes:

[Objeto](https://es.mathworks.com/help/examples/vision/win64/FeatureBasedObjectDetectionExample_01.png) - [Contexto](https://es.mathworks.com/help/examples/vision/win64/FeatureBasedObjectDetectionExample_02.png)

Tendremos que descargar estas imágenes, guardarlas como **objeto.jpg** y **contexto.jpg** y recortarles los bordes blancos para que la práctica sea más natural. Cuando tengamos esto hecho, deberemos importar una a una nuestras imágenes en MATLAB. Podemos hacerlo arrastrando el fichero al entorno y dándole a *Finish* cuando las detecte. Luego, ejecutaremos los siguientes comandos:

**objeto = rgb2gray(objeto);**

**contexto = rgb2gray(contexto);**

### Detección.

MATLAB nos ofrece varias funciones para detectar las características con el detector que nosotros queramos. Estas funciones son:
- detectSIFTFeatures()
- detectSURFFeatures()
- detectORBFeatures()
- detectBRISKFeatures()

Nosotros usaremos **detectSIFTFeatures()** de la siguiente manera:

puntos_objeto = detectSIFTFeatures(objeto);

puntos_contexto = detectSIFTFeatures(contexto);

Para mostrar los datos recogidos de la imagen podemos usar los siguientes comandos:

**figure;**

**imshow(objeto);**

**hold on;**

**plot(selectStrongest(puntos_objeto,100));**

Con esto mostraremos la imagen **objeto.jpg** con los cien puntos de las características más fuertes que se hayan detectado. Podremos hacer lo mismo para el contexto:

**figure;**

**imshow(contexto);**

**hold on;**

**plot(selectStrongest(puntos_contexto,300));**

Aquí mostraremos los 300 puntos más fuertes que se han detectado en el contexto.

![image](Images/ImagenesDescriptores/puntos_objeto.png)

![image](Images/ImagenesDescriptores/puntos_contexto.png)

### Extracción.

Deberemos extraer las características de la imagen. Este proceso lo haremos con dos comandos que guardarán en una tupla característica y punto todas las características que se encuentren. Los comandos son:

**[features_contexto, puntos_contexto] = extractFeatures(contexto, puntos_contexto);**

**[features_objeto, puntos_objeto] = extractFeatures(objeto, puntos_objeto);**

![image](Images/ImagenesDescriptores/variable_features.png)

### *Matching*.

Ahora que tenemos las características extraídas podemos realizar el *matching*:

**parejas_objeto = matchFeatures(features_objeto,features_contexto);**

Con el comando anterior hemos emparejado las características iguales que se han encontrado en la imagen del objeto con las que han sido halladas en la imagen del contexto.

![image](Images/ImagenesDescriptores/variable_parejas_objeto.png)

Podemos mostrar estas características emparejadas de la siguiente forma:

**puntos_emparejados_objeto = puntos_objeto(parejas_objeto(:, 1), :);**

**puntos_emparejados_contexto = puntos_contexto(parejas_objeto(:, 2), :);**

**figure;**

**showMatchedFeatures(objeto,contexto,puntos_emparejados_objeto,puntos_emparejados_contexto,'montage');**

El comando anterior sacará la siguiente figura por pantalla:

![image](Images/ImagenesDescriptores/puntos_emparejados_objetos.png)

### Refinado.

A menudo nos salen puntos emparejados con otros que no tienen nada que ver. Son errores que los algoritmos pueden cometer. Para solucionar esto, podemos refinar el resultado con el siguiente comando:

**[tform,inlierIdx]=estgeotform2d(puntos_emparejados_objeto,puntos_emparejados_contexto,'affine');**

![image](Images/ImagenesDescriptores/refinado.png)

### Localización.

Por último, podemos localizar el objeto en la imagen con las características que hemos obtenido. Ejecutaremos los siguientes comandos:

**boxPolygon =** 

 **[1, 1;...                           % top-left**
 **size(objeto, 2), 1;...                 % top-right**
 **size(objeto, 2), size(objeto, 1);... % bottom-right**
 **1, size(objeto, 1);...                 % bottom-left**
 **1, 1];                   % top-left again to close the polygon**

**newBoxPolygon = transformPointsForward(tform,boxPolygon);**

**figure;**

**imshow(contexto);**

**hold on;**

**line(newBoxPolygon(:, 1), newBoxPolygon(:, 2), Color='y');**

Obtendremos como salida la siguiente imagen:

![image](Images/ImagenesDescriptores/localizacion.png)

### Comparación con otros decsriptores.

Podemos hacer el mismo proceso con los otros descriptores, pero cambiando el comando en el que hicimos **detectSIFTFeatures()** por su correspondiente detector.

#### SURF

![image](Images/ImagenesDescriptores/puntos_emparejados_objetos_SURF.png)

#### ORB

![image](Images/ImagenesDescriptores/puntos_emparejados_objetos_ORB.png)

#### BRISK

![image](Images/ImagenesDescriptores/puntos_emparejados_objetos_BRISK.png)

### Observaciones.

Es importante destacar que la calidad importa. A continucación, observamos, aplicando el descriptor BRISK, lo que ocurre si tenemos las mismas imágenes pero con mejor calidad.

![image](Images/ImagenesDescriptores/puntos_emparejados_objetos_BRISK_2.png)

Como observamos, se pueden emparejar muchos más puntos y características en una imagen de mejor calidad.
